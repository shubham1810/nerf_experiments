{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "import glob\n",
                "import numpy as np\n",
                "import os\n",
                "from PIL import Image\n",
                "from torchvision import transforms as T\n",
                "\n",
                "from datasets.ray_utils import *\n",
                "from datasets.colmap_utils import \\\n",
                "    read_cameras_binary, read_images_binary, read_points3d_binary"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/home2/sdokania/.local/lib/python3.8/site-packages/kornia/augmentation/augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "\n",
                "def normalize(v):\n",
                "    \"\"\"Normalize a vector.\"\"\"\n",
                "    return v/np.linalg.norm(v)\n",
                "\n",
                "\n",
                "def average_poses(poses):\n",
                "    \"\"\"\n",
                "    Calculate the average pose, which is then used to center all poses\n",
                "    using @center_poses. Its computation is as follows:\n",
                "    1. Compute the center: the average of pose centers.\n",
                "    2. Compute the z axis: the normalized average z axis.\n",
                "    3. Compute axis y': the average y axis.\n",
                "    4. Compute x' = y' cross product z, then normalize it as the x axis.\n",
                "    5. Compute the y axis: z cross product x.\n",
                "    \n",
                "    Note that at step 3, we cannot directly use y' as y axis since it's\n",
                "    not necessarily orthogonal to z axis. We need to pass from x to y.\n",
                "\n",
                "    Inputs:\n",
                "        poses: (N_images, 3, 4)\n",
                "\n",
                "    Outputs:\n",
                "        pose_avg: (3, 4) the average pose\n",
                "    \"\"\"\n",
                "    # 1. Compute the center\n",
                "    center = poses[..., 3].mean(0) # (3)\n",
                "\n",
                "    # 2. Compute the z axis\n",
                "    z = normalize(poses[..., 2].mean(0)) # (3)\n",
                "\n",
                "    # 3. Compute axis y' (no need to normalize as it's not the final output)\n",
                "    y_ = poses[..., 1].mean(0) # (3)\n",
                "\n",
                "    # 4. Compute the x axis\n",
                "    x = normalize(np.cross(y_, z)) # (3)\n",
                "\n",
                "    # 5. Compute the y axis (as z and x are normalized, y is already of norm 1)\n",
                "    y = np.cross(z, x) # (3)\n",
                "\n",
                "    pose_avg = np.stack([x, y, z, center], 1) # (3, 4)\n",
                "\n",
                "    return pose_avg\n",
                "\n",
                "\n",
                "def center_poses(poses):\n",
                "    \"\"\"\n",
                "    Center the poses so that we can use NDC.\n",
                "    See https://github.com/bmild/nerf/issues/34\n",
                "\n",
                "    Inputs:\n",
                "        poses: (N_images, 3, 4)\n",
                "\n",
                "    Outputs:\n",
                "        poses_centered: (N_images, 3, 4) the centered poses\n",
                "        pose_avg: (3, 4) the average pose\n",
                "    \"\"\"\n",
                "\n",
                "    pose_avg = average_poses(poses) # (3, 4)\n",
                "    pose_avg_homo = np.eye(4)\n",
                "    pose_avg_homo[:3] = pose_avg # convert to homogeneous coordinate for faster computation\n",
                "                                 # by simply adding 0, 0, 0, 1 as the last row\n",
                "    last_row = np.tile(np.array([0, 0, 0, 1]), (len(poses), 1, 1)) # (N_images, 1, 4)\n",
                "    poses_homo = \\\n",
                "        np.concatenate([poses, last_row], 1) # (N_images, 4, 4) homogeneous coordinate\n",
                "\n",
                "    poses_centered = np.linalg.inv(pose_avg_homo) @ poses_homo # (N_images, 4, 4)\n",
                "    poses_centered = poses_centered[:, :3] # (N_images, 3, 4)\n",
                "\n",
                "    return poses_centered, pose_avg\n",
                "\n",
                "\n",
                "def create_spiral_poses(radii, focus_depth, n_poses=120):\n",
                "    \"\"\"\n",
                "    Computes poses that follow a spiral path for rendering purpose.\n",
                "    See https://github.com/Fyusion/LLFF/issues/19\n",
                "    In particular, the path looks like:\n",
                "    https://tinyurl.com/ybgtfns3\n",
                "\n",
                "    Inputs:\n",
                "        radii: (3) radii of the spiral for each axis\n",
                "        focus_depth: float, the depth that the spiral poses look at\n",
                "        n_poses: int, number of poses to create along the path\n",
                "\n",
                "    Outputs:\n",
                "        poses_spiral: (n_poses, 3, 4) the poses in the spiral path\n",
                "    \"\"\"\n",
                "\n",
                "    poses_spiral = []\n",
                "    for t in np.linspace(0, 4*np.pi, n_poses+1)[:-1]: # rotate 4pi (2 rounds)\n",
                "        # the parametric function of the spiral (see the interactive web)\n",
                "        center = np.array([np.cos(t), -np.sin(t), -np.sin(0.5*t)]) * radii\n",
                "\n",
                "        # the viewing z axis is the vector pointing from the @focus_depth plane\n",
                "        # to @center\n",
                "        z = normalize(center - np.array([0, 0, -focus_depth]))\n",
                "        \n",
                "        # compute other axes as in @average_poses\n",
                "        y_ = np.array([0, 1, 0]) # (3)\n",
                "        x = normalize(np.cross(y_, z)) # (3)\n",
                "        y = np.cross(z, x) # (3)\n",
                "\n",
                "        poses_spiral += [np.stack([x, y, z, center], 1)] # (3, 4)\n",
                "\n",
                "    return np.stack(poses_spiral, 0) # (n_poses, 3, 4)\n",
                "\n",
                "\n",
                "def create_spheric_poses(radius, n_poses=120):\n",
                "    \"\"\"\n",
                "    Create circular poses around z axis.\n",
                "    Inputs:\n",
                "        radius: the (negative) height and the radius of the circle.\n",
                "\n",
                "    Outputs:\n",
                "        spheric_poses: (n_poses, 3, 4) the poses in the circular path\n",
                "    \"\"\"\n",
                "    def spheric_pose(theta, phi, radius):\n",
                "        trans_t = lambda t : np.array([\n",
                "            [1,0,0,0],\n",
                "            [0,1,0,-0.9*t],\n",
                "            [0,0,1,t],\n",
                "            [0,0,0,1],\n",
                "        ])\n",
                "\n",
                "        rot_phi = lambda phi : np.array([\n",
                "            [1,0,0,0],\n",
                "            [0,np.cos(phi),-np.sin(phi),0],\n",
                "            [0,np.sin(phi), np.cos(phi),0],\n",
                "            [0,0,0,1],\n",
                "        ])\n",
                "\n",
                "        rot_theta = lambda th : np.array([\n",
                "            [np.cos(th),0,-np.sin(th),0],\n",
                "            [0,1,0,0],\n",
                "            [np.sin(th),0, np.cos(th),0],\n",
                "            [0,0,0,1],\n",
                "        ])\n",
                "\n",
                "        c2w = rot_theta(theta) @ rot_phi(phi) @ trans_t(radius)\n",
                "        c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
                "        return c2w[:3]\n",
                "\n",
                "    spheric_poses = []\n",
                "    for th in np.linspace(0, 2*np.pi, n_poses+1)[:-1]:\n",
                "        spheric_poses += [spheric_pose(th, -np.pi/5, radius)] # 36 degree view downwards\n",
                "    return np.stack(spheric_poses, 0)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "class LLFF2DDataset(Dataset):\n",
                "    def __init__(self, root_dir, split='train', img_wh=(504, 378), spheric_poses=False, val_num=1):\n",
                "        \"\"\"\n",
                "        spheric_poses: whether the images are taken in a spheric inward-facing manner\n",
                "                       default: False (forward-facing)\n",
                "        val_num: number of val images (used for multigpu training, validate same image for all gpus)\n",
                "        \"\"\"\n",
                "        self.root_dir = root_dir\n",
                "        self.split = split\n",
                "        self.img_wh = img_wh\n",
                "        self.spheric_poses = spheric_poses\n",
                "        self.val_num = max(1, val_num) # at least 1\n",
                "        self.define_transforms()\n",
                "\n",
                "        self.read_meta()\n",
                "        self.white_back = False\n",
                "\n",
                "    def read_meta(self):\n",
                "        # Step 1: rescale focal length according to training resolution\n",
                "        camdata = read_cameras_binary(os.path.join(self.root_dir, 'sparse/0/cameras.bin'))\n",
                "        H = camdata[1].height\n",
                "        W = camdata[1].width\n",
                "        self.focal = camdata[1].params[0] * self.img_wh[0]/W\n",
                "\n",
                "        # Step 2: correct poses\n",
                "        # read extrinsics (of successfully reconstructed images)\n",
                "        imdata = read_images_binary(os.path.join(self.root_dir, 'sparse/0/images.bin'))\n",
                "        perm = np.argsort([imdata[k].name for k in imdata])\n",
                "        # read successfully reconstructed images and ignore others\n",
                "        self.image_paths = [os.path.join(self.root_dir, 'images', name)\n",
                "                            for name in sorted([imdata[k].name for k in imdata])]\n",
                "        w2c_mats = []\n",
                "        bottom = np.array([0, 0, 0, 1.]).reshape(1, 4)\n",
                "        for k in imdata:\n",
                "            im = imdata[k]\n",
                "            R = im.qvec2rotmat()\n",
                "            t = im.tvec.reshape(3, 1)\n",
                "            w2c_mats += [np.concatenate([np.concatenate([R, t], 1), bottom], 0)]\n",
                "        w2c_mats = np.stack(w2c_mats, 0)\n",
                "        poses = np.linalg.inv(w2c_mats)[:, :3] # (N_images, 3, 4) cam2world matrices\n",
                "        \n",
                "        # read bounds\n",
                "        self.bounds = np.zeros((len(poses), 2)) # (N_images, 2)\n",
                "        pts3d = read_points3d_binary(os.path.join(self.root_dir, 'sparse/0/points3D.bin'))\n",
                "        pts_world = np.zeros((1, 3, len(pts3d))) # (1, 3, N_points)\n",
                "        visibilities = np.zeros((len(poses), len(pts3d))) # (N_images, N_points)\n",
                "        for i, k in enumerate(pts3d):\n",
                "            pts_world[0, :, i] = pts3d[k].xyz\n",
                "            for j in pts3d[k].image_ids:\n",
                "                visibilities[j-1, i] = 1\n",
                "        # calculate each point's depth w.r.t. each camera\n",
                "        # it's the dot product of \"points - camera center\" and \"camera frontal axis\"\n",
                "        self.points = pts_world\n",
                "        depths = ((pts_world-poses[..., 3:4])*poses[..., 2:3]).sum(1) # (N_images, N_points)\n",
                "        for i in range(len(poses)):\n",
                "            visibility_i = visibilities[i]\n",
                "            zs = depths[i][visibility_i==1]\n",
                "            self.bounds[i] = [np.percentile(zs, 0.1), np.percentile(zs, 99.9)]\n",
                "        # permute the matrices to increasing order\n",
                "        poses = poses[perm]\n",
                "        self.bounds = self.bounds[perm]\n",
                "        \n",
                "        # COLMAP poses has rotation in form \"right down front\", change to \"right up back\"\n",
                "        # See https://github.com/bmild/nerf/issues/34\n",
                "        poses = np.concatenate([poses[..., 0:1], -poses[..., 1:3], poses[..., 3:4]], -1)\n",
                "        self.poses, _ = center_poses(poses)\n",
                "        distances_from_center = np.linalg.norm(self.poses[..., 3], axis=1)\n",
                "        val_idx = np.argmin(distances_from_center) # choose val image as the closest to\n",
                "                                                   # center image\n",
                "\n",
                "        # Step 3: correct scale so that the nearest depth is at a little more than 1.0\n",
                "        # See https://github.com/bmild/nerf/issues/34\n",
                "        near_original = self.bounds.min()\n",
                "        scale_factor = near_original*0.75 # 0.75 is the default parameter\n",
                "                                          # the nearest depth is at 1/0.75=1.33\n",
                "        self.bounds /= scale_factor\n",
                "        self.poses[..., 3] /= scale_factor\n",
                "\n",
                "        # ray directions for all pixels, same for all images (same H, W, focal)\n",
                "        self.directions = \\\n",
                "            get_ray_directions(self.img_wh[1], self.img_wh[0], self.focal) # (H, W, 3)\n",
                "            \n",
                "        if self.split == 'train': # create buffer of all rays and rgb data\n",
                "                                  # use first N_images-1 to train, the LAST is val\n",
                "            self.all_rays = []\n",
                "            self.all_rgbs = []\n",
                "            for i, image_path in enumerate(self.image_paths):\n",
                "                if i == val_idx: # exclude the val image\n",
                "                    continue\n",
                "                c2w = torch.FloatTensor(self.poses[i])\n",
                "\n",
                "                img = Image.open(image_path).convert('RGB')\n",
                "                # assert img.size[1]*self.img_wh[0] == img.size[0]*self.img_wh[1], \\\n",
                "                #     f'''{image_path} has different aspect ratio than img_wh, \n",
                "                #         please check your data!'''\n",
                "                img = img.resize(self.img_wh, Image.LANCZOS)\n",
                "                img = self.transform(img) # (3, h, w)\n",
                "                img = img.permute(1, 2, 0)\n",
                "                # img = img.view(3, -1).permute(1, 0) # (h*w, 3) RGB\n",
                "                self.all_rgbs += [img]\n",
                "                \n",
                "                rays_o, rays_d = get_rays(self.directions, c2w) # both (h*w, 3)\n",
                "                if not self.spheric_poses:\n",
                "                    near, far = 0, 1\n",
                "                    rays_o, rays_d = get_ndc_rays(self.img_wh[1], self.img_wh[0],\n",
                "                                                  self.focal, 1.0, rays_o, rays_d)\n",
                "                                     # near plane is always at 1.0\n",
                "                                     # near and far in NDC are always 0 and 1\n",
                "                                     # See https://github.com/bmild/nerf/issues/34\n",
                "                else:\n",
                "                    near = self.bounds.min()\n",
                "                    far = min(8 * near, self.bounds.max()) # focus on central object only\n",
                "\n",
                "                self.all_rays += [torch.cat([rays_o, rays_d, \n",
                "                                             near*torch.ones_like(rays_o[:, :1]),\n",
                "                                             far*torch.ones_like(rays_o[:, :1])],\n",
                "                                             1).reshape(self.img_wh[1], self.img_wh[0], -1)] # (h*w, 8)\n",
                "                                 \n",
                "            # self.all_rays = torch.cat(self.all_rays, 0) # ((N_images-1)*h*w, 8)\n",
                "            # self.all_rgbs = torch.cat(self.all_rgbs, 0) # ((N_images-1)*h*w, 3)\n",
                "        \n",
                "        elif self.split == 'val':\n",
                "            print('val image is', self.image_paths[val_idx])\n",
                "            self.val_idx = val_idx\n",
                "\n",
                "        else: # for testing, create a parametric rendering path\n",
                "            if self.split.endswith('train'): # test on training set\n",
                "                self.poses_test = self.poses\n",
                "            elif not self.spheric_poses:\n",
                "                focus_depth = 3.5 # hardcoded, this is numerically close to the formula\n",
                "                                  # given in the original repo. Mathematically if near=1\n",
                "                                  # and far=infinity, then this number will converge to 4\n",
                "                radii = np.percentile(np.abs(self.poses[..., 3]), 90, axis=0)\n",
                "                self.poses_test = create_spiral_poses(radii, focus_depth)\n",
                "            else:\n",
                "                radius = 1.1 * self.bounds.min()\n",
                "                self.poses_test = create_spheric_poses(radius)\n",
                "\n",
                "    def define_transforms(self):\n",
                "        self.transform = T.ToTensor()\n",
                "\n",
                "    def __len__(self):\n",
                "        if self.split == 'train':\n",
                "            return len(self.all_rays)\n",
                "        if self.split == 'val':\n",
                "            return self.val_num\n",
                "        if self.split == 'test_train':\n",
                "            return len(self.poses)\n",
                "        return len(self.poses_test)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        if self.split == 'train': # use data in the buffers\n",
                "            sample = {'rays': self.all_rays[idx],\n",
                "                      'rgbs': self.all_rgbs[idx]}\n",
                "\n",
                "        else:\n",
                "            if self.split == 'val':\n",
                "                c2w = torch.FloatTensor(self.poses[self.val_idx])\n",
                "            elif self.split == 'test_train':\n",
                "                c2w = torch.FloatTensor(self.poses[idx])\n",
                "            else:\n",
                "                c2w = torch.FloatTensor(self.poses_test[idx])\n",
                "\n",
                "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
                "            if not self.spheric_poses:\n",
                "                near, far = 0, 1\n",
                "                rays_o, rays_d = get_ndc_rays(self.img_wh[1], self.img_wh[0],\n",
                "                                              self.focal, 1.0, rays_o, rays_d)\n",
                "            else:\n",
                "                near = self.bounds.min()\n",
                "                far = min(8 * near, self.bounds.max())\n",
                "\n",
                "            rays = torch.cat([rays_o, rays_d, \n",
                "                              near*torch.ones_like(rays_o[:, :1]),\n",
                "                              far*torch.ones_like(rays_o[:, :1])],\n",
                "                              1).reshape(self.img_wh[1], self.img_wh[0], -1) # (h*w, 8)\n",
                "\n",
                "            sample = {'rays': rays,\n",
                "                      'c2w': c2w}\n",
                "\n",
                "            if self.split in ['val', 'test_train']:\n",
                "                if self.split == 'val':\n",
                "                    idx = self.val_idx\n",
                "                img = Image.open(self.image_paths[idx]).convert('RGB')\n",
                "                img = img.resize(self.img_wh, Image.LANCZOS)\n",
                "                img = self.transform(img) # (3, h, w)\n",
                "                img = img.permute(1, 2, 0)\n",
                "                # img = img.view(3, -1).permute(1, 0) # (h*w, 3)\n",
                "                sample['rgbs'] = img\n",
                "\n",
                "        return sample"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "train_dataset = LLFF2DDataset('/ssd_scratch/cvit/shubham/nerf_llff_data/fern/', 'train', img_wh=(400, 400))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "len(train_dataset.all_rays), train_dataset.all_rays[0].shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(19, torch.Size([400, 400, 8]))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 26
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "len(train_dataset.all_rgbs), train_dataset.all_rgbs[0].shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(19, torch.Size([400, 400, 3]))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 27
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.3 64-bit"
        },
        "interpreter": {
            "hash": "e1a0b73f138b8e46c1ab7ba22a2d68cfb24a87eea0df27b153684073fdda7be0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}