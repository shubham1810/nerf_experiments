{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import skimage.io as sio\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import copy\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nusc = NuScenes(version='v1.0-mini', dataroot='../../shubham/', verbose=True) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.656 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def translation2transform(vec):\n",
    "    i = np.eye(4)\n",
    "    i[:3, -1] = vec\n",
    "    return i"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "scene_path = \"../../sus_data/even_smaller_scene/\"\n",
    "img_path = os.path.join(scene_path, 'images')\n",
    "pcd_path = os.path.join(scene_path, 'pcd')\n",
    "\n",
    "os.makedirs(img_path, exist_ok=True)\n",
    "os.makedirs(pcd_path, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# First define list of images\n",
    "all_cams = ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT']#, 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT']\n",
    "\n",
    "# intrinsic = {}\n",
    "# extrinsic = {}\n",
    "\n",
    "# for ix in all_cams:\n",
    "#     intrinsic[ix] = []\n",
    "#     extrinsic[ix] = []\n",
    "\n",
    "full_dict = []\n",
    "\n",
    "downscale = 4\n",
    "\n",
    "\n",
    "for n_idx in tqdm.tqdm_notebook(range(22)):\n",
    "    # Get a particular sample\n",
    "    my_sample = nusc.sample[n_idx]\n",
    "    my_sample_token = my_sample['token']\n",
    "    sample_record = nusc.get('sample', my_sample_token)\n",
    "\n",
    "    # Get point sensor token\n",
    "    pointsensor_token = sample_record['data']['LIDAR_TOP']\n",
    "\n",
    "    # Get point cloud\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = os.path.join(nusc.dataroot, pointsensor['filename'])\n",
    "    # pc = LidarPointCloud.from_file(pcl_path)\n",
    "\n",
    "    # Iterate through the list\n",
    "    for cam_ix in all_cams:\n",
    "        img_folder = os.path.join(img_path, cam_ix)\n",
    "        # os.makedirs(img_folder, exist_ok=True)\n",
    "        img_save_path = os.path.join(\"{}_{}\".format(img_folder, str(n_idx).zfill(3)) + '.jpg')\n",
    "\n",
    "        # Get the camera token\n",
    "        camera_token = sample_record['data'][cam_ix]\n",
    "        cam = nusc.get('sample_data', camera_token)\n",
    "        im = Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "        w, h = im.size\n",
    "        \n",
    "        res_w, res_h = w//downscale, h//downscale\n",
    "        res_mat = np.eye(3)\n",
    "        res_mat[0, 0] = res_mat[1, 1] = 1/downscale\n",
    "        \n",
    "        # Save the image to the location\n",
    "        im = im.resize((res_w, res_h), Image.LANCZOS)\n",
    "        im.save(img_save_path)\n",
    "        # print(img_save_path)\n",
    "        \n",
    "        \n",
    "        # Commpute the calibration matrices (Don't save right now)\n",
    "        cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "        R1 = Quaternion(cs_record['rotation']).transformation_matrix\n",
    "        T1 = translation2transform(np.array(cs_record['translation']))\n",
    "\n",
    "        # Second step: transform from ego to the global frame.\n",
    "        poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "        R2 = Quaternion(poserecord['rotation']).transformation_matrix\n",
    "        T2 = translation2transform(np.array(poserecord['translation']))\n",
    "\n",
    "        # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "        poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "        T3 = translation2transform(-np.array(poserecord['translation']))\n",
    "        R3 = Quaternion(poserecord['rotation']).transformation_matrix.T\n",
    "\n",
    "        # Fourth step: transform from ego into the camera.\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        T4 = translation2transform(-np.array(cs_record['translation']))\n",
    "        R4 = Quaternion(cs_record['rotation']).transformation_matrix.T\n",
    "        \n",
    "        # intrinsic[cam_ix].append(np.array(cs_record['camera_intrinsic']).flatten())\n",
    "        # extrinsic[cam_ix].append((np.linalg.inv(R4 @ T4 @ R3 @ T3)).flatten())\n",
    "        intrinsic = (res_mat @ np.array(cs_record['camera_intrinsic'])).flatten()\n",
    "        extrinsic = (np.linalg.inv(R4 @ T4 @ R3 @ T3)).flatten()\n",
    "        \n",
    "        # np.set_printoptions(suppress=True)\n",
    "        \n",
    "        data_dict = {\n",
    "            'intrinsic': list(intrinsic),\n",
    "            'extrinsic': list(extrinsic),\n",
    "            'file_name': \"{}/{}_{}\".format(\"images\", cam_ix, str(n_idx).zfill(3))\n",
    "        }\n",
    "        \n",
    "        full_dict.append(data_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-b4dfa843894c>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for n_idx in tqdm.tqdm_notebook(range(22)):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e9879691fb64d58a83ddea5d9f822ea"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dict = []\n",
    "val_dict = []\n",
    "test_dict = []\n",
    "\n",
    "for ix in full_dict:\n",
    "    r = np.random.random()\n",
    "    if r < 0.12:\n",
    "        val_dict.append(ix)\n",
    "    elif r >= 0.12 and r < 0.18:\n",
    "        test_dict.append(ix)\n",
    "    else:\n",
    "        train_dict.append(ix)\n",
    "\n",
    "print(len(train_dict), len(val_dict), len(test_dict))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52 9 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "final = {\n",
    "    \"frames\": train_dict\n",
    "}\n",
    "f = open(os.path.join(scene_path, \"transforms_train.json\"), 'w')\n",
    "json.dump(final, f)\n",
    "f.close()\n",
    "\n",
    "final = {\n",
    "    \"frames\": test_dict\n",
    "}\n",
    "f = open(os.path.join(scene_path, \"transforms_test.json\"), 'w')\n",
    "json.dump(final, f)\n",
    "f.close()\n",
    "\n",
    "final = {\n",
    "    \"frames\": val_dict\n",
    "}\n",
    "f = open(os.path.join(scene_path, \"transforms_val.json\"), 'w')\n",
    "json.dump(final, f)\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms as T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from ray_utils import *\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class NusDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", img_wh=(1600, 900), downscale=1, perturbation=[]):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.downscale = downscale\n",
    "        \n",
    "        self.img_wh = img_wh\n",
    "        self.define_transforms()\n",
    "        self.perturbation = perturbation\n",
    "        \n",
    "        w, h = self.img_wh\n",
    "        self.res_wh = (w // self.downscale, h // self.downscale)\n",
    "        self.res_mat = np.eye(3)\n",
    "        self.res_mat[0, 0] = self.res_mat[1, 1] = 1/self.downscale\n",
    "        \n",
    "        self.read_meta()\n",
    "        self.white_back = True\n",
    "    \n",
    "    def read_meta(self):\n",
    "        with open(os.path.join(self.root_dir,\n",
    "                               f\"transforms_{self.split.split('_')[-1]}.json\"), 'r') as f:\n",
    "            self.meta = json.load(f)\n",
    "        \n",
    "        self.near = 2.0\n",
    "        self.far = 10.0\n",
    "        self.bounds = np.array([self.near, self.far])\n",
    "        w, h = self.res_wh\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.all_rays = []\n",
    "            self.all_rgbs = []\n",
    "            for t, frame in enumerate(self.meta['frames']):\n",
    "                # if t >= 40:\n",
    "                #     break\n",
    "                K = self.res_mat @ np.array(frame['intrinsic']).reshape((3, 3))\n",
    "                pose = np.array(frame['extrinsic']).reshape((4, 4))[:3, :4]\n",
    "                c2w = torch.FloatTensor(pose)\n",
    "                \n",
    "                image_path = os.path.join(self.root_dir, \"{}.jpg\".format(frame['file_name']))\n",
    "                img = Image.open(image_path)\n",
    "                img = img.resize(self.res_wh, Image.LANCZOS)\n",
    "                img = self.transform(img).view(3, -1).permute(1, 0)\n",
    "                self.all_rgbs += [img]\n",
    "                \n",
    "                # Now for the rays\n",
    "                directions = get_ray_directions(h, w, K)\n",
    "                rays_o, rays_d = get_rays(directions, c2w)\n",
    "                rays_t = t * torch.ones(len(rays_o), 1)\n",
    "                \n",
    "                # Add to list of all rays\n",
    "                self.all_rays += [torch.cat([rays_o, rays_d,\n",
    "                                             self.near*torch.ones_like(rays_o[:, :1]),\n",
    "                                             self.far*torch.ones_like(rays_o[:, :1]),\n",
    "                                             rays_t],\n",
    "                                             1)]\n",
    "            self.all_rays = torch.cat(self.all_rays, 0)\n",
    "            self.all_rgbs = torch.cat(self.all_rgbs, 0)\n",
    "    \n",
    "    def define_transforms(self):\n",
    "        self.transform = T.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return len(self.all_rays)\n",
    "        if self.split == 'val':\n",
    "            return 8\n",
    "        return len(self.meta['frames'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train':\n",
    "            sample = {'rays': self.all_rays[idx, :8],\n",
    "                      'ts': self.all_rays[idx, 8].long(),\n",
    "                      'rgbs': self.all_rgbs[idx]}\n",
    "        else:\n",
    "            # create data for each image separately\n",
    "            frame = self.meta['frames'][idx]\n",
    "            t = 0\n",
    "            \n",
    "            K = self.res_mat @ np.array(frame['intrinsic']).reshape((3, 3))\n",
    "            pose = np.array(frame['extrinsic']).reshape((4, 4))[:3, :4]\n",
    "            c2w = torch.FloatTensor(pose)\n",
    "            \n",
    "            image_path = os.path.join(self.root_dir, \"{}.jpg\".format(frame['file_name']))\n",
    "            img = Image.open(image_path)\n",
    "            img = img.resize(self.res_wh, Image.LANCZOS)\n",
    "            img = self.transform(img).view(3, -1).permute(1, 0)\n",
    "            valid_mask = (img[0]>0).flatten()\n",
    "            \n",
    "            w, h = self.res_wh\n",
    "            \n",
    "            # Now for the rays\n",
    "            directions = get_ray_directions(h, w, K)\n",
    "            rays_o, rays_d = get_rays(directions, c2w)\n",
    "            rays_t = t * torch.ones(len(rays_o), 1)\n",
    "            \n",
    "            rays = torch.cat([rays_o, rays_d,\n",
    "                              self.near*torch.ones_like(rays_o[:, :1]),\n",
    "                              self.far*torch.ones_like(rays_o[:, :1])],\n",
    "                             1)\n",
    "            \n",
    "            sample = {\n",
    "                'rays': rays,\n",
    "                'ts': t * torch.ones(len(rays), dtype=torch.long),\n",
    "                'rgbs': img,\n",
    "                'c2w': c2w,\n",
    "                'valid_mask': valid_mask\n",
    "            }\n",
    "            \n",
    "            if self.split == 'train_test':\n",
    "                sample['original_rgbs'] = img\n",
    "                sample['original_valid_mask'] = valid_mask\n",
    "        return sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df = NusDataset(scene_path, downscale=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df.all_rgbs.shape[0] // (1024*3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9000000"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "fr =  df.meta['frames'][0]\n",
    "print(np.array(fr['intrinsic']).reshape((3, 3)))\n",
    "print(np.array(fr['extrinsic']).reshape((4, 4)))\n",
    "fr"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1266.41720305    0.          816.26701974]\n",
      " [   0.         1266.41720305  491.50706579]\n",
      " [   0.            0.            1.        ]]\n",
      "[[  -0.94016521   -0.01558255   -0.34036239  410.87244105]\n",
      " [   0.33999683    0.02209463   -0.940167   1179.57081167]\n",
      " [   0.02217038   -0.99963444   -0.01547459    1.49367752]\n",
      " [   0.            0.            0.            1.        ]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'intrinsic': [1266.417203046554,\n",
       "  0.0,\n",
       "  816.2670197447984,\n",
       "  0.0,\n",
       "  1266.417203046554,\n",
       "  491.50706579294757,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0],\n",
       " 'extrinsic': [-0.9401652123589346,\n",
       "  -0.015582548073058936,\n",
       "  -0.34036239167339144,\n",
       "  410.8724410536508,\n",
       "  0.33999683492988736,\n",
       "  0.022094631607521308,\n",
       "  -0.9401669955341908,\n",
       "  1179.5708116715798,\n",
       "  0.02217037906169335,\n",
       "  -0.9996344389073835,\n",
       "  -0.015474586992294873,\n",
       "  1.4936775157046203,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0],\n",
       " 'file_name': 'images/CAM_FRONT_000'}"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}